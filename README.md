# Machine-Learning-Deep-Learning-Summary
#### &#160; &#160; &#160; &#160;**机器学习/深度学习知识点总结**

&#160; &#160; &#160; &#160;关于在ML/DL方面的知识点做一个总结，有些知识点会给出个人非常喜欢的博客链接详解，有些会给出我自己的博客。

&#160; &#160; &#160; &#160;***持续更新中...***

#### &#160; &#160; &#160; &#160;一、机器学习

#### &#160; &#160; &#160; &#160;1. Logistic Regression

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/logistic_regression.png)

&#160; &#160; &#160; &#160;Logistic Regression是在Linear Regression基础上加了一个logistic函数，这样可以得到一个概率值，从而将回归变成了分类。

&#160; &#160; &#160; &#160;那么，**Logistic Regression和Linear Regression的区别是什么呢？**

&#160; &#160; &#160; &#160;区别如下：

&#160; &#160; &#160; &#160;**· 线性回归是拟合，逻辑斯蒂回归是分类。**

&#160; &#160; &#160; &#160;**· 线性回归使用最小二乘法最小化预测值和真实值之间的差距，逻辑斯蒂回归使用最大似然估计计算使得数据出现的可能性最大的参数。**

&#160; &#160; &#160; &#160;**· 逻辑斯蒂回归是在线性回归的基础上，加了一层logistic函数(又叫sigmoid函数)做了一层转换，使得输出压缩到0-1之间，这样0-0.5为1类，0.5-1为2类。**

&#160; &#160; &#160; &#160;那么继续思考，**为什么使用logistic函数？**

&#160; &#160; &#160; &#160;比如说，假设x>0时y=1(1类)，x<=0时y=0(2类)，那么画这个函数，就是一个阶跃函数，在0点这个分界点处发生了阶跃，有从0到1的突变，导致这点不连续，在数学上处理起来不太方便。

&#160; &#160; &#160; &#160;这时，引入logistic函数，如下：

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/logistic_func.png)

&#160; &#160; &#160; &#160;优点如下：

&#160; &#160; &#160; &#160;**· 输入范围是−∞→+∞ ，而输出范围在0~1之间，满足我们对输入和输出的要求。**

&#160; &#160; &#160; &#160;**· logsitic函数接近于单位阶跃函数，而且是一个连续函数，且单调可微，优化起来更加简单。**

#### &#160; &#160; &#160; &#160;2. SVM

&#160; &#160; &#160; &#160;可以看这篇博客的详解：https://www.cnblogs.com/steven-yang/p/5658362.html

#### &#160; &#160; &#160; &#160;3. KNN

&#160; &#160; &#160; &#160;KNN的核心思想：将样本分到离它最相似的的样本所属的类。

&#160; &#160; &#160; &#160;主要步骤：

&#160; &#160; &#160; &#160;对未知类别属性的样本依次执行如下步骤：

&#160; &#160; &#160; &#160;(1).计算未知类别属性的样本点与已知类别数据集中每个点的距离；

&#160; &#160; &#160; &#160;(2).按照距离递增的方式排序；

&#160; &#160; &#160; &#160;(3).选取与当前未知类别样本点距离最小的k个点；

&#160; &#160; &#160; &#160;(4).确定前k个点所在类别的出现频率；

&#160; &#160; &#160; &#160;(5).当前未知样本点的预测分类类别为前k个点中出现频率最高的类别。

&#160; &#160; &#160; &#160;**优点**：精度高，对异常值不敏感，无数据输入设定。

&#160; &#160; &#160; &#160;**缺点**：计算复杂度高，空间复杂度高。

&#160; &#160; &#160; &#160;由于需要计算样本间的距离，因此需要依赖距离定义，常用的距离函数有欧氏距离，Mahalanobis距离，Bhattacharyya距离等，关于机器学习中的距离函数，可以看这篇博客：https://www.cnblogs.com/ronny/p/4080442.html。

&#160; &#160; &#160; &#160;KNN是一种判别模型，既支持分类问题，也支持回归问题，是一种非线性模型。KNN天然的支持多分类问题，KNN算法没有训练过程，是一种基于实例的算法。

&#160; &#160; &#160; &#160;那么，**K值该怎么选择呢？**

&#160; &#160; &#160; &#160;k在KNN算法中是一个很重要的参数，k的选取将严重影响最终的结果。

&#160; &#160; &#160; &#160;如下图，当k取3时，结果为红色三角形，而当k取5时，结果为蓝色正方形。

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/KNN-k.png)

&#160; &#160; &#160; &#160;k值设置过小会降低分类精度，设置过大且测试样本属于训练集中包含数据较少的类，则会增加噪声，降低分类效果。

&#160; &#160; &#160; &#160;通常，k值的设置采用交叉检验的方式(以k=1为基准)。

&#160; &#160; &#160; &#160;经验规则：k值一般低于训练样本数的平方根。

#### &#160; &#160; &#160; &#160;4. K-均值聚类算法(K-means)

&#160; &#160; &#160; &#160;K-means的核心思想：将未知样本分配到离它最近的类中心所属的类，类中心由属于这个类的所有样本确定。

&#160; &#160; &#160; &#160;K-means是一种无监督的聚类算法，在实现时，先随机初始化每个类的类中心，然后计算样本与每个类的类中心的距离，将其分配到最近的那个类，然后再根据这种分配方案重新计算每个类的类中心。

&#160; &#160; &#160; &#160;样本之间的距离定义与KNN一样，具体可参考KNN。K-means在实现时主要考虑以下几个问题：

&#160; &#160; &#160; &#160;(1).**类中心向量的初始化**。第一种方案：一般采用随机初始化，从样本集中随机选择k个样本作为初始类中心。第二种方案：随机划分，将所有样本随机分配给k个类中的一个，然后再按照这种分配方案计算各个类的类中心。

&#160; &#160; &#160; &#160;(2).**参数k的设定**。和KNN一样，k值设定至关重要，可以根据先验知识人工指定一个值，或者由算法自己确定。大部分情况下，还是需要算法来确定一个最佳k值，高维数据可视化之后根据人工判断类别数难度太大。这里主要讨论两种最优k值的选取，一是**手肘法**，二是**轮廓系数法**，手肘法较轮廓系数法更佳，所以主要讲解一下手肘法。

&#160; &#160; &#160; &#160;**手肘法的核心思想**：随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE(每个点到其所属的簇中心的距离的平方和)自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。

&#160; &#160; &#160; &#160;SSE下降速度突然变得平缓的拐点即认为是最佳的k值，如下图所示：

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/kmeans-elbow.jpg)

&#160; &#160; &#160; &#160;具体k值选取和轮廓系数法可查看这篇博客：https://blog.csdn.net/qq_15738501/article/details/79036255

&#160; &#160; &#160; &#160;(3)**迭代终止的判定条件**。一般做法是计算本次迭代后的类中心和上一次迭代时的类中心之间的距离，如果小于设定阈值，则算法终止。

&#160; &#160; &#160; &#160;**优点**：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(1).算法快速、简单，易解释

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(2).聚类效果中上

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(3).适用于高维

&#160; &#160; &#160; &#160;**缺点**：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(1).对离群点敏感，对噪声点和孤立点很敏感

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(2).k值的选择很重要

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(3).初始聚类中心的选择，不同的初始点选择可能导致完全不同的聚类效果

#### &#160; &#160; &#160; &#160;5. 基于概率论的分类方法：朴素贝叶斯

&#160; &#160; &#160; &#160;朴素贝叶斯的核心思想：将样本判定为后验概率最大的类，它直接用贝叶斯公式解决分类问题。

&#160; &#160; &#160; &#160;朴素贝叶斯分类器有两个假设：一、每个特征之间相互独立，二、每个特征同等重要。其实这两个假设均有问题，这也是"朴素"这一词的由来。

&#160; &#160; &#160; &#160;假设样本的特征向量为x，类别标签为y，根据贝叶斯公式，样本属于每个类的条件概率（后验概率）为：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/bayes_1.png)

&#160; &#160; &#160; &#160;分母p(x)对所有类都是相同的，分类的规则是将样本归到后验概率最大的那个类，不需要计算准确的概率值，只需要知道属于哪个类的概率最大即可，这样可以忽略掉分母。分类器的判别函数为：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/bayes_2.png)

&#160; &#160; &#160; &#160;在实现贝叶斯分类器时，需要知道每个类的条件概率分布p(x|y)，即先验概率。一般假设样本服从正态分布。训练时确定先验概率分布的参数，一般用最大似然估计，即最大化对数似然函数。

&#160; &#160; &#160; &#160;如果假设特征向量的各个分量之间相互独立，则称为朴素贝叶斯分类器，此时的分类判别函数为：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/bayes_3.png)

&#160; &#160; &#160; &#160;实现时可以分为特征分量是离散变量和连续变量两种情况。贝叶斯分分类器是一种生成模型，可以处理多分类问题，是一种非线性模型。

#### &#160; &#160; &#160; &#160;6. 决策树

&#160; &#160; &#160; &#160;决策树是一种基于规则的方法，它用一组嵌套的规则进行预测。在树的每个决策节点处，根据判断结果进入一个分支，反复执行这种操作直到到达叶子节点，得到预测结果。这些规则通过训练得到，而不是人工制定的。

&#160; &#160; &#160; &#160;决策树既可以用于分类问题，也可以用于回归问题。

&#160; &#160; &#160; &#160;**决策树的构造(以分类为例)：**

&#160; &#160; &#160; &#160;决策树的构造是一个递归的选择最优特征，并根据该特征对训练数据集进行分割，使得每个子数据集有一个最好的分类的过程。

&#160; &#160; &#160; &#160;(1).构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按这一最优特征将训练数据集分割成多个子集。

&#160; &#160; &#160; &#160;(2).如果这些子集已经能够被正确分类，那么就构建叶节点，将这些子集分到所对应的叶节点中去。

&#160; &#160; &#160; &#160;(3).如果存在子集不能够被正确的分类，那么就在这些子集里面选择新的最优特征，继续对其进行分割，这样递归进行，直至所有子集都能够被正确的分类，或者没有合适的特征为止，这样就生成了一颗决策树。

&#160; &#160; &#160; &#160;**最优特征的选取：** 根据信息增益来选取，划分数据集前后信息发生的变化称为信息增益，获得信息增益最高的特征就是最好的选择，简单来说，信息增益度量的原则就是划分之后能将无序数据变得更加有序的程度。

&#160; &#160; &#160; &#160;**特点：**

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;**·** 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;**·** 缺点：可能会产生过度匹配的问题。

&#160; &#160; &#160; &#160;决策树可以处理属性缺失问题，采用的方法是使用替代分裂规则。为了防止过拟合，可以对树进行剪枝，让模型变得更简单。

#### &#160; &#160; &#160; &#160;7. Bagging和Boosting

&#160; &#160; &#160; &#160;Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法，即将弱分类器组装成强分类器的方法。

&#160; &#160; &#160; &#160;**(1).Bagging**

&#160; &#160; &#160; &#160;Bagging即套袋法，它是一种有放回的抽样方法（可能抽到重复的样本）,其算法过程如下：

&#160; &#160; &#160; &#160;A)从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集（k个训练集之间是相互独立的）。

&#160; &#160; &#160; &#160;B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

&#160; &#160; &#160; &#160;C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

&#160; &#160; &#160; &#160;**(2).Boosting**

&#160; &#160; &#160; &#160;Boosting的主要思想是将弱分类器组装成一个强分类器。

&#160; &#160; &#160; &#160;关于Boosting的两个核心问题：

&#160; &#160; &#160; &#160;1）在每一轮如何改变训练数据的权值或概率分布？

&#160; &#160; &#160; &#160;通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

&#160; &#160; &#160; &#160;2）通过什么方式来组合弱分类器？

&#160; &#160; &#160; &#160;通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。

&#160; &#160; &#160; &#160;而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

&#160; &#160; &#160; &#160;**(3).Bagging，Boosting二者之间的区别**

&#160; &#160; &#160; &#160;Bagging和Boosting的区别：

&#160; &#160; &#160; &#160;1）样本选择上：

&#160; &#160; &#160; &#160;Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

&#160; &#160; &#160; &#160;Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化，而权值是根据上一轮的分类结果进行调整。

&#160; &#160; &#160; &#160;2）样例权重：

&#160; &#160; &#160; &#160;Bagging：使用均匀取样，每个样例的权重相等

&#160; &#160; &#160; &#160;Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

&#160; &#160; &#160; &#160;3）预测函数：

&#160; &#160; &#160; &#160;Bagging：所有预测函数的权重相等。

&#160; &#160; &#160; &#160;Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

&#160; &#160; &#160; &#160;4）并行计算：

&#160; &#160; &#160; &#160;Bagging：各个预测函数可以并行生成

&#160; &#160; &#160; &#160;Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

&#160; &#160; &#160; &#160;**(4).总结**

&#160; &#160; &#160; &#160;这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。

&#160; &#160; &#160; &#160;下面是将决策树与这些算法框架进行结合所得到的新的算法：

&#160; &#160; &#160; &#160;1）Bagging + 决策树 = 随机森林

&#160; &#160; &#160; &#160;2）AdaBoost + 决策树 = 提升树

&#160; &#160; &#160; &#160;3）Gradient Boosting + 决策树 = GBDT

&#160; &#160; &#160; &#160;主要来自这篇博客：http://www.cnblogs.com/liuwu265/p/4690486.html

#### &#160; &#160; &#160; &#160;8. 随机森林

&#160; &#160; &#160; &#160;++随机森林是一种集成学习算法，它的基本单元是决策树，是Bagging算法的具体实现。++ 集成学习是机器学习中的一种思想，而不是某一具体算法，它通过多个模型的组合形成一个精度更高的模型，参与组合的模型称为弱学习器。在预测时使用这些弱学习器模型联合进行预测，训练时需要依次训练出这些弱学习器。

&#160; &#160; &#160; &#160;随机森林用**随机且有放回抽样**（Bootstrap抽样）构成出的样本集训练多棵决策树，训练决策树的每个节点时只使用了随机抽样的部分特征。预测时，对于分类问题，一个测试样本会送到每一棵决策树中进行预测，然后投票，得票最多的类为最终分类结果。对于回归问题，随机森林的预测输出是所有决策树输出的均值。

&#160; &#160; &#160; &#160;**为什么要随机抽样训练集？**

&#160; &#160; &#160; &#160;如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要。

&#160; &#160; &#160; &#160;**为什么要有放回地抽样？**

&#160; &#160; &#160; &#160;如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，也就是说每棵树训练出来都是有很大的差异的。而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。

&#160; &#160; &#160; &#160;由于使用了随机抽样，随机森林泛化性能一般比较好，可以有效的降低模型的方差，不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。

&#160; &#160; &#160; &#160;随机森林分类效果（错误率）与两个因素有关：

&#160; &#160; &#160; &#160; **·** 森林中任意两棵树的相关性：相关性越大，错误率越大；

&#160; &#160; &#160; &#160; **·** 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

&#160; &#160; &#160; &#160;推荐博客：http://www.cnblogs.com/maybe2030/p/4585705.html

#### &#160; &#160; &#160; &#160;9. AdaBoost

&#160; &#160; &#160; &#160;AdaBoost全称为Adaptive Boosting(自适应boosting)，取名规则和AdaGrad一致。前面已经讲了Boosting算法，Boosting在对待错分的样本时，其对应的样本权值会越大，在对待弱分类器时，弱分类器的分类误差越小，那么该弱分类器对应的权值也会越大。基于此思想，AdaBoost就是以自适应调节样本和弱分类器的权重这一目的出发的。

&#160; &#160; &#160; &#160;所以自适应体现在以下两点：

&#160; &#160; &#160; &#160;**·** AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。然后，再根据所采用的一些基本机器学习算法进行学习，比如逻辑回归。

&#160; &#160; &#160; &#160;**·** AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。

&#160; &#160; &#160; &#160;算法流程如下：

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/AdaBoost.png)

&#160; &#160; &#160; &#160;标准的AdaBoost算法是一种判别模型，只能支持二分类问题。它的改进型可以处理多分类问题。












