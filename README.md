# Machine-Learning-Deep-Learning-Summary
#### &#160; &#160; &#160; &#160;**机器学习/深度学习知识点总结**

&#160; &#160; &#160; &#160;关于在ML/DL方面的知识点做一个总结，有些知识点会给出个人非常喜欢的博客链接详解，有些会给出我自己的博客。

&#160; &#160; &#160; &#160;***持续更新中...***

#### &#160; &#160; &#160; &#160;一、机器学习

#### &#160; &#160; &#160; &#160;1. Logistic Regression

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/logistic_regression.png)

&#160; &#160; &#160; &#160;Logistic Regression是在Linear Regression基础上加了一个logistic函数，这样可以得到一个概率值，从而将回归变成了分类。

&#160; &#160; &#160; &#160;那么，**Logistic Regression和Linear Regression的区别是什么呢？**

&#160; &#160; &#160; &#160;区别如下：

&#160; &#160; &#160; &#160;**· 线性回归是拟合，逻辑斯蒂回归是分类。**

&#160; &#160; &#160; &#160;**· 线性回归使用最小二乘法最小化预测值和真实值之间的差距，逻辑斯蒂回归使用最大似然估计计算使得数据出现的可能性最大的参数。**

&#160; &#160; &#160; &#160;**· 逻辑斯蒂回归是在线性回归的基础上，加了一层logistic函数(又叫sigmoid函数)做了一层转换，使得输出压缩到0-1之间，这样0-0.5为1类，0.5-1为2类。**

&#160; &#160; &#160; &#160;那么继续思考，**为什么使用logistic函数？**

&#160; &#160; &#160; &#160;比如说，假设x>0时y=1(1类)，x<=0时y=0(2类)，那么画这个函数，就是一个阶跃函数，在0点这个分界点处发生了阶跃，有从0到1的突变，导致这点不连续，在数学上处理起来不太方便。

&#160; &#160; &#160; &#160;这时，引入logistic函数，如下：

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/logistic_func.png)

&#160; &#160; &#160; &#160;优点如下：

&#160; &#160; &#160; &#160;**· 输入范围是−∞→+∞ ，而输出范围在0~1之间，满足我们对输入和输出的要求。**

&#160; &#160; &#160; &#160;**· logsitic函数接近于单位阶跃函数，而且是一个连续函数，且单调可微，优化起来更加简单。**

#### &#160; &#160; &#160; &#160;2. SVM

&#160; &#160; &#160; &#160;可以看这篇博客的详解：https://www.cnblogs.com/steven-yang/p/5658362.html

#### &#160; &#160; &#160; &#160;3. KNN

&#160; &#160; &#160; &#160;KNN的核心思想：将样本分到离它最相似的的样本所属的类。

&#160; &#160; &#160; &#160;主要步骤：

&#160; &#160; &#160; &#160;对未知类别属性的样本依次执行如下步骤：

&#160; &#160; &#160; &#160;(1).计算未知类别属性的样本点与已知类别数据集中每个点的距离；

&#160; &#160; &#160; &#160;(2).按照距离递增的方式排序；

&#160; &#160; &#160; &#160;(3).选取与当前未知类别样本点距离最小的k个点；

&#160; &#160; &#160; &#160;(4).确定前k个点所在类别的出现频率；

&#160; &#160; &#160; &#160;(5).当前未知样本点的预测分类类别为前k个点中出现频率最高的类别。

&#160; &#160; &#160; &#160;**优点**：精度高，对异常值不敏感，无数据输入设定。

&#160; &#160; &#160; &#160;**缺点**：计算复杂度高，空间复杂度高。

&#160; &#160; &#160; &#160;由于需要计算样本间的距离，因此需要依赖距离定义，常用的距离函数有欧氏距离，Mahalanobis距离，Bhattacharyya距离等，关于机器学习中的距离函数，可以看这篇博客：https://www.cnblogs.com/ronny/p/4080442.html。

&#160; &#160; &#160; &#160;KNN是一种判别模型，既支持分类问题，也支持回归问题，是一种非线性模型。KNN天然的支持多分类问题，KNN算法没有训练过程，是一种基于实例的算法。

&#160; &#160; &#160; &#160;那么，**K值该怎么选择呢？**

&#160; &#160; &#160; &#160;k在KNN算法中是一个很重要的参数，k的选取将严重影响最终的结果。

&#160; &#160; &#160; &#160;如下图，当k取3时，结果为红色三角形，而当k取5时，结果为蓝色正方形。

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/KNN-k.png)

&#160; &#160; &#160; &#160;k值设置过小会降低分类精度，设置过大且测试样本属于训练集中包含数据较少的类，则会增加噪声，降低分类效果。

&#160; &#160; &#160; &#160;通常，k值的设置采用交叉检验的方式(以k=1为基准)。

&#160; &#160; &#160; &#160;经验规则：k值一般低于训练样本数的平方根。

#### &#160; &#160; &#160; &#160;4. K-均值聚类算法(K-means)

&#160; &#160; &#160; &#160;K-means的核心思想：将未知样本分配到离它最近的类中心所属的类，类中心由属于这个类的所有样本确定。

&#160; &#160; &#160; &#160;K-means是一种无监督的聚类算法，在实现时，先随机初始化每个类的类中心，然后计算样本与每个类的类中心的距离，将其分配到最近的那个类，然后再根据这种分配方案重新计算每个类的类中心。

&#160; &#160; &#160; &#160;样本之间的距离定义与KNN一样，具体可参考KNN。K-means在实现时主要考虑以下几个问题：

&#160; &#160; &#160; &#160;(1).**类中心向量的初始化**。第一种方案：一般采用随机初始化，从样本集中随机选择k个样本作为初始类中心。第二种方案：随机划分，将所有样本随机分配给k个类中的一个，然后再按照这种分配方案计算各个类的类中心。

&#160; &#160; &#160; &#160;(2).**参数k的设定**。和KNN一样，k值设定至关重要，可以根据先验知识人工指定一个值，或者由算法自己确定。大部分情况下，还是需要算法来确定一个最佳k值，高维数据可视化之后根据人工判断类别数难度太大。这里主要讨论两种最优k值的选取，一是**手肘法**，二是**轮廓系数法**，手肘法较轮廓系数法更佳，所以主要讲解一下手肘法。

&#160; &#160; &#160; &#160;**手肘法的核心思想**：随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE(每个点到其所属的簇中心的距离的平方和)自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。

&#160; &#160; &#160; &#160;SSE下降速度突然变得平缓的拐点即认为是最佳的k值，如下图所示：

&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/kmeans-elbow.jpg)

&#160; &#160; &#160; &#160;具体k值选取和轮廓系数法可查看这篇博客：https://blog.csdn.net/qq_15738501/article/details/79036255

&#160; &#160; &#160; &#160;(3)**迭代终止的判定条件**。一般做法是计算本次迭代后的类中心和上一次迭代时的类中心之间的距离，如果小于设定阈值，则算法终止。

&#160; &#160; &#160; &#160;**优点**：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(1).算法快速、简单，易解释

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(2).聚类效果中上

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(3).适用于高维

&#160; &#160; &#160; &#160;**缺点**：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(1).对离群点敏感，对噪声点和孤立点很敏感

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(2).k值的选择很重要

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;(3).初始聚类中心的选择，不同的初始点选择可能导致完全不同的聚类效果

#### &#160; &#160; &#160; &#160;5. 基于概率论的分类方法：朴素贝叶斯

&#160; &#160; &#160; &#160;朴素贝叶斯的核心思想：将样本判定为后验概率最大的类，它直接用贝叶斯公式解决分类问题。

&#160; &#160; &#160; &#160;朴素贝叶斯分类器有两个假设：一、每个特征之间相互独立，二、每个特征同等重要。其实这两个假设均有问题，这也是"朴素"这一词的由来。

&#160; &#160; &#160; &#160;假设样本的特征向量为x，类别标签为y，根据贝叶斯公式，样本属于每个类的条件概率（后验概率）为：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/bayes_1.png)

&#160; &#160; &#160; &#160;分母p(x)对所有类都是相同的，分类的规则是将样本归到后验概率最大的那个类，不需要计算准确的概率值，只需要知道属于哪个类的概率最大即可，这样可以忽略掉分母。分类器的判别函数为：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/bayes_2.png)

&#160; &#160; &#160; &#160;在实现贝叶斯分类器时，需要知道每个类的条件概率分布p(x|y)，即先验概率。一般假设样本服从正态分布。训练时确定先验概率分布的参数，一般用最大似然估计，即最大化对数似然函数。

&#160; &#160; &#160; &#160;如果假设特征向量的各个分量之间相互独立，则称为朴素贝叶斯分类器，此时的分类判别函数为：

&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;![image](https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary/raw/master/img/bayes_3.png)

&#160; &#160; &#160; &#160;实现时可以分为特征分量是离散变量和连续变量两种情况。贝叶斯分分类器是一种生成模型，可以处理多分类问题，是一种非线性模型。

